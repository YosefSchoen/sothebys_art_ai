{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%idle_timeout 2880\n",
    "%glue_version 4.0\n",
    "%worker_type G.1X\n",
    "%number_of_workers 5\n",
    "\n",
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "  \n",
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)"
   ],
   "id": "9a88a5bc2a9c766a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "dyf = glueContext.create_dynamic_frame.from_catalog(database='sothebys-db', table_name='scraped_data_raw')\n",
    "dyf.printSchema()"
   ],
   "id": "d3bc51bd64ab5705"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = dyf.toDF()\n",
    "df.show()"
   ],
   "id": "8c684042fc2e5084"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Extract the top-level DataFrame\n",
    "collection_df = df.select(\"page_id\", \"collection_id\", \"category\", \"title\", \"details\", \"price\", \"link\", \"partition_0\")\n",
    "print(\"Top-level DataFrame:\")\n",
    "collection_df.show()"
   ],
   "id": "1b545485382edabd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pyspark.sql.functions import split, udf, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType, TimestampType, StructField\n",
    "\n",
    "collection_df = collection_df.withColumn(\"info\", split(col(\"details\"), '\\|'))\n",
    "\n",
    "\n",
    "# Define a UDF to get the last element from the split array\n",
    "def get_last_element(arr):\n",
    "    return arr[-1] if arr else None\n",
    "\n",
    "get_last_element_udf = udf(get_last_element, StringType())\n",
    "\n",
    "# Apply the UDF to get the 'city' column\n",
    "collection_df = collection_df.withColumn(\"city\", get_last_element_udf(col(\"info\")))\n",
    "\n",
    "# Drop the intermediate 'info' column if it's no longer needed\n",
    "collection_df = collection_df.drop(\"info\")\n",
    "\n",
    "# Show the DataFrame\n",
    "collection_df.show()"
   ],
   "id": "95256d18a014c3d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from datetime import date, datetime\n",
    "\n",
    "months = {\n",
    "    \"January\": 1,\n",
    "    \"February\": 2,\n",
    "    \"March\": 3,\n",
    "    \"April\": 4,\n",
    "    \"May\": 5,\n",
    "    \"June\": 6,\n",
    "    \"July\": 7,\n",
    "    \"August\": 8,\n",
    "    \"September\": 9,\n",
    "    \"October\": 10,\n",
    "    \"November\": 11,\n",
    "    \"December\": 12\n",
    "}\n",
    "\n",
    "def get_date(_date):\n",
    "    if len(_date) == 3:\n",
    "        start_day_end_day = _date[0].split('–')\n",
    "        start_day = ''\n",
    "        end_day = ''\n",
    "        \n",
    "        if len(start_day_end_day) == 1:\n",
    "            start_day = int(start_day_end_day[0])\n",
    "            end_day = int(start_day_end_day[0])\n",
    "        elif  len(start_day_end_day) == 2:\n",
    "            start_day = int(start_day_end_day[0])\n",
    "            end_day = int(start_day_end_day[1])\n",
    "            \n",
    "        start_month = months[_date[1]]\n",
    "        end_month = months[_date[1]]\n",
    "        year = int(_date[2])\n",
    "        \n",
    "        start_date = date(day=start_day, month=start_month, year=year)\n",
    "        end_date = date(day=end_day, month=end_month, year=year)\n",
    "        return [start_date, end_date]\n",
    "    \n",
    "    elif len(_date) == 4:\n",
    "        start_day = int(_date[0])\n",
    "        start_month = months[_date[1].split('–')[0]]\n",
    "        end_day = int(_date[1].split('–')[1])\n",
    "        end_month = months[_date[2]]\n",
    "        year = int(_date[3])\n",
    "        \n",
    "        start_date = date(day=start_day, month=start_month, year=year)\n",
    "        end_date = date(day=end_day, month=end_month, year=year)\n",
    "        return [start_date, end_date]\n",
    "\n",
    "def get_time(_time):\n",
    "    time_format = \"%I:%M %p\"\n",
    "    \n",
    "    if len(_time) == 3:\n",
    "        time_of_day = _time[0]\n",
    "        am_pm = _time[1]\n",
    "        time = datetime.strptime(time_of_day+ ' ' + am_pm, time_format).time()\n",
    "        time_zone = _time[2]\n",
    "        return [time, time_zone]\n",
    "    return [datetime.time, '']\n",
    "\n",
    "def get_date_time(_date_time):\n",
    "    if len(_date_time) == 1:\n",
    "        start_date, end_date = get_date(_date_time[0].strip().split())\n",
    "        return start_date, end_date, ''\n",
    "    \n",
    "    elif len(_date_time) == 2:\n",
    "        start_date, end_date = get_date(_date_time[0].strip().split())\n",
    "        _time, time_zone = get_time(_date_time[1].strip().split())\n",
    "        \n",
    "        \n",
    "        return datetime.combine(start_date, _time), datetime.combine(end_date, _time), time_zone\n",
    "    \n",
    "    return '', ''"
   ],
   "id": "ee7b4d87deafc619"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from datetime import date, datetime\n",
    "\n",
    "# Register the UDFs\n",
    "get_date_time_udf = udf(\n",
    "    lambda dt: get_date_time(dt.split('|')[:-1]), \n",
    "    StructType(\n",
    "        [\n",
    "            StructField(\"start_date\", DateType(), True),\n",
    "            StructField(\"end_date\", DateType(), True),\n",
    "            StructField(\"time_zone\", StringType(), True)\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "collection_df = collection_df.withColumn(\"date_time_struct\", get_date_time_udf(col(\"details\")))\n",
    "\n",
    "# Split the struct into separate columns\n",
    "collection_df = collection_df.withColumn(\n",
    "    \"start_date\", \n",
    "    col(\"date_time_struct.start_date\")\n",
    ").withColumn(\n",
    "    \"end_date\", \n",
    "    col(\"date_time_struct.end_date\")\n",
    ").withColumn(\n",
    "    \"time_zone\",\n",
    "    col(\"date_time_struct.time_zone\")\n",
    ").drop(\"date_time_struct\", \"details\")\n",
    "\n",
    "collection_df.show()"
   ],
   "id": "aa5fa13f3502f516"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def get_price(_price_str):\n",
    "    if _price_str != 'n/a':\n",
    "        _price = _price_str.split(':')[1]\n",
    "        _price, _currency = _price.split()\n",
    "        return int(_price.replace(',', '')), _currency\n",
    "    return -1, ''\n",
    "\n",
    "get_price_udf = udf(lambda dt: get_price(dt), StructType([\n",
    "    StructField(\"price\", IntegerType(), True),\n",
    "    StructField(\"currency\", StringType(), True)\n",
    "]))\n",
    "\n",
    "\n",
    "collection_df = collection_df.withColumn(\"price_currency_struct\", get_price_udf(col(\"price\")))\n",
    "\n",
    "collection_df = collection_df.withColumn(\n",
    "    \"price\", \n",
    "    col(\"price_currency_struct.price\")\n",
    ").withColumn(\n",
    "    \"currency\", col(\"price_currency_struct.currency\")\n",
    ").drop(\"price_currency_struct\")\n",
    "\n",
    "collection_df = collection_df.withColumn(\"price\", col(\"price\").cast(\"integer\"))\n",
    "collection_df.show()"
   ],
   "id": "48cb7e3361bfbd72"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Extract the address nested DataFrame\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "items_df = df.withColumn(\"items\", explode(\"items\")).select('items.*')\n",
    "print(\"Address DataFrame:\")\n",
    "items_df.show()"
   ],
   "id": "7594e6eaf82cdbac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "items_df = items_df.withColumn('partition_0', items_df.page_id)\n",
    "items_df.show()"
   ],
   "id": "e240e0cccc1e7c35"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pyspark.sql.functions import split, col, lit, when\n",
    "\n",
    "# Fill missing values for 'author' and 'price_sold'\n",
    "items_df = items_df.withColumn(\n",
    "    'author', \n",
    "    when(col('author').isNull(),lit('-1.-1'))\n",
    "    .otherwise(col('author'))\n",
    ")\n",
    "items_df = items_df.withColumn(\n",
    "    'price_sold', \n",
    "    when(col('price_sold').isNull(),lit('-1 -1'))\n",
    "    .otherwise(col('price_sold'))\n",
    ")\n",
    "\n",
    "# Split 'author' into 'item_id' and 'author'\n",
    "items_df = items_df.withColumn(\n",
    "    'item_id', \n",
    "    split(col('author'), '\\\\.')\n",
    "    .getItem(0)\n",
    ")\n",
    "\n",
    "items_df = items_df.withColumn(\n",
    "    'author', \n",
    "    split(col('author'), '\\\\.')\n",
    "    .getItem(1)\n",
    ")\n",
    "\n",
    "# Split 'price_sold' into 'price_sold' and 'currency'\n",
    "items_df = items_df.withColumn(\n",
    "    'currency', \n",
    "    split(col('price_sold'), ' ')\n",
    "    .getItem(1)\n",
    ")\n",
    "\n",
    "items_df = items_df.withColumn(\n",
    "    'price_sold', \n",
    "    split(col('price_sold'), ' ')\n",
    "    .getItem(0)\n",
    ")\n",
    "\n",
    "# Split 'estimated_price' into 'low_estimate' and 'high_estimate'\n",
    "items_df = items_df.withColumn(\n",
    "    'low_estimate', \n",
    "    split(col('estimated_price'), '-')\n",
    "    .getItem(0)\n",
    ")\n",
    "\n",
    "items_df = items_df.withColumn(\n",
    "    'high_estimate', \n",
    "    split(\n",
    "        split(col('estimated_price'), '-').getItem(1), ' '\n",
    "    ).getItem(1))\n",
    "\n",
    "# Drop the 'estimated_price' column\n",
    "items_df = items_df.drop('estimated_price')\n",
    "\n",
    "# Show the transformed DataFrame\n",
    "items_df.show()"
   ],
   "id": "699665b389277c5c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "items_df = items_df.withColumn(\"price_sold\", col(\"price_sold\").cast(\"integer\"))\n",
    "items_df = items_df.withColumn(\"item_id\", col(\"item_id\").cast(\"integer\"))\n",
    "items_df = items_df.withColumn(\"low_estimate\", col(\"low_estimate\").cast(\"integer\"))\n",
    "items_df = items_df.withColumn(\"high_estimate\", col(\"high_estimate\").cast(\"integer\"))\n",
    "items_df.printSchema()"
   ],
   "id": "933d2160f9bc8d20"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from awsglue.dynamicframe import DynamicFrame\n",
    "\n",
    "collection_dyf = DynamicFrame.fromDF(collection_df, glueContext, \"collections\")\n",
    "\n",
    "s3output = glueContext.getSink(\n",
    "  path=\"s3://scraped-data-clean/collections\",\n",
    "  connection_type=\"s3\",\n",
    "  updateBehavior=\"UPDATE_IN_DATABASE\",\n",
    "  partitionKeys=['partition_0'],\n",
    "  compression=\"snappy\",\n",
    "  enableUpdateCatalog=True,\n",
    "  transformation_ctx=\"s3output\",\n",
    ")\n",
    "\n",
    "s3output.setCatalogInfo(\n",
    "  catalogDatabase=\"sothebys-db\", catalogTableName=\"collections\"\n",
    ")\n",
    "\n",
    "s3output.setFormat(\"glueparquet\")\n",
    "\n",
    "s3output.writeFrame(collection_dyf)"
   ],
   "id": "3d2ff8a91760f1c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "items_dyf = DynamicFrame.fromDF(items_df, glueContext, \"items\")\n",
    "\n",
    "s3output = glueContext.getSink(\n",
    "  path=\"s3://scraped-data-clean/items\",\n",
    "  connection_type=\"s3\",\n",
    "  updateBehavior=\"UPDATE_IN_DATABASE\",\n",
    "  partitionKeys=['partition_0'],\n",
    "  compression=\"snappy\",\n",
    "  enableUpdateCatalog=True,\n",
    "  transformation_ctx=\"s3output\",\n",
    ")\n",
    "\n",
    "s3output.setCatalogInfo(\n",
    "  catalogDatabase=\"sothebys-db\", catalogTableName=\"items\"\n",
    ")\n",
    "\n",
    "s3output.setFormat(\"glueparquet\")\n",
    "\n",
    "s3output.writeFrame(items_dyf)"
   ],
   "id": "a418b425b0da9ab5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
